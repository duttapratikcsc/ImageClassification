{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_BuildingDataPipeLines.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/duttapratikcsc/NPTEL/blob/master/Initials/02_BuildingDataPipeLines.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ0o_-ZRN72P",
        "colab_type": "text"
      },
      "source": [
        "The `tf.data` API enables you to build complex input pipelines from simple,\n",
        "reusable pieces. For example, the pipeline for an image model might aggregate\n",
        "data from files in a distributed file system, apply random perturbations to each\n",
        "image, and merge randomly selected images into a batch for training. The\n",
        "pipeline for a text model might involve extracting symbols from raw text data,\n",
        "converting them to embedding identifiers with a lookup table, and batching\n",
        "together sequences of different lengths. The `tf.data` API makes it possible to\n",
        "handle large amounts of data, read from different data formats, and perform\n",
        "complex transformations.\n",
        "\n",
        "The `tf.data` API introduces a `tf.data.Dataset` abstraction that represents a\n",
        "sequence of elements, in which each element consists of one or more components.\n",
        "For example, in an image pipeline, an element might be a single training\n",
        "example, with a pair of tensor components representing the image and its label.\n",
        "\n",
        "There are two distinct ways to create a dataset:\n",
        "\n",
        "*   A data **source** constructs a `Dataset` from data stored in memory or in\n",
        "    one or more files.\n",
        "\n",
        "*   A data **transformation** constructs a dataset from one or more\n",
        "    `tf.data.Dataset` objects."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJZ3ESFNA2x3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wqyf-BDOFSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wxgHQX3OLTQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(precision=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8TvcedmOPIw",
        "colab_type": "text"
      },
      "source": [
        "## Basic mechanics\n",
        "<a id=\"basic-mechanics\"/>\n",
        "\n",
        "To create an input pipeline, you must start with a data *source*. For example,\n",
        "to construct a `Dataset` from data in memory, you can use\n",
        "`tf.data.Dataset.from_tensors()` or `tf.data.Dataset.from_tensor_slices()`.\n",
        "Alternatively, if your input data is stored in a file in the recommended\n",
        "TFRecord format, you can use `tf.data.TFRecordDataset()`.\n",
        "\n",
        "Once you have a `Dataset` object, you can *transform* it into a new `Dataset` by\n",
        "chaining method calls on the `tf.data.Dataset` object. For example, you can\n",
        "apply per-element transformations such as `Dataset.map()`, and multi-element\n",
        "transformations such as `Dataset.batch()`. See the documentation for\n",
        "`tf.data.Dataset` for a complete list of transformations.\n",
        "\n",
        "The `Dataset` object is a Python iterable. This makes it possible to consume its\n",
        "elements using a for loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUJlKtd_OP_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_mqHAedOT3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for elem in dataset:\n",
        "  print(elem.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGe7eav0OWr1",
        "colab_type": "text"
      },
      "source": [
        "Or by explicitly creating a Python iterator using `iter` and consuming its\n",
        "elements using `next`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGSqol0ZOUdw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}